{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data for this Project\n",
    "This project involved gathering of data from three different sources as listed below. For each of the data source a different method of data gathering was used namely:-  \n",
    ">**Importing data via csv**  \n",
    ">**Using requests to download data off internet**  \n",
    ">**Scrape data from an API**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three data sources**\n",
    ">**Enhanced Twitter Archive**  \n",
    "The WeRateDogs Twitter archive provided by Udacity. This contains basic tweet data for all 5000+ of their tweets, but not everything.I manually downloaded this file manually by clicking the following link: twitter_archive_enhanced.csv.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Image Predictions File**  \n",
    "The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: image_predictions.tsv  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Data via the Twitter API**   \n",
    "Each tweet's retweet count and favorite (\"like\") count at minimum, and any additional data you find interesting. Using the tweet IDs in the WeRateDogs Twitter archive, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. Each tweet's JSON data should be written to its own line. Then read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count.  \n",
    "> Though a tweeter developer account has been opened succesfully and was able to create a api object, couldn't download twitter data as they stored as missing tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data\n",
    "Data gathered were stored in dataframes and programatically assessed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Issues\n",
    "1. There are 23 occurances of denominator entries which are !10. Replace them with 10.\n",
    "2. Decimal values for numerator in text should be included in rating_numerator.\n",
    "3. Rename columns: time_stamp to tweet_time, text to tweets, names to dog's name.\n",
    "4. Make 'source' column readable.\n",
    "5. Erroneous data types, tweet_id - string, times stamp - date and time, life_stage - category.\n",
    "6. Remove rows with no images - expanded_url.\n",
    "7. Dogs name entered as None and lowercase should be removed.\n",
    "8. Drop columns that are not necessary for the analysis; in_reply_to_status_id, in_reply_to_user_id,retweeted_status_user_id retweeted_status_timestamp.\n",
    "9. There are 181 retweets which need to be removed.\n",
    "10. There are 78 tweet replies which need to be removed.\n",
    "\n",
    "## Tidiness Issues\n",
    "1. Combine columns \"doggo\", \"floofer\", \"pupper\", and \"puppo\" to one life_stage column.\n",
    "2. Three datasets should be merged as they are part of the same observational unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "I used my knowledge of python and searching over the internet i.e. google, stackoverflow, stackabuse etc for references and possible guideance to resolve the above mentioned issues to the best of my knowledge. There was lot trial and error for difficult cases where regular expressions had to be used but at the same time some things for instance dropping the not so useful columns was pretty straight forward.  \n",
    "Time constraint to analyse and retrieve missing dogs name, breed and valid rating from the 'text' column to make the analysis more meaningful.\n",
    "\n",
    "Finally, once the data was ready I analyzed it using visualizations as documented in act_report.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'act_report.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
